{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # import numpy library\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_in, n_out, ini_type='plain'):\n",
    "\n",
    "    params = dict()  # initialize empty dictionary of neural net parameters W and b\n",
    "\n",
    "    if ini_type == 'plain':\n",
    "        params['W'] = np.random.randn(n_out, n_in) *0.01  # set weights 'W' to small random gaussian\n",
    "    elif ini_type == 'xavier':\n",
    "        params['W'] = np.random.randn(n_out, n_in) / (np.sqrt(n_in))  # set variance of W to 1/n\n",
    "    elif ini_type == 'he':\n",
    "        params['W'] = np.random.randn(n_out, n_in) * np.sqrt(2/n_in)  # set variance of W to 2/n\n",
    "\n",
    "    params['b'] = np.zeros((n_out, 1))    # set bias 'b' to zeros\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "\n",
    "\n",
    "    def __init__(self, input_shape, n_out, ini_type=\"plain\"):\n",
    "\n",
    "\n",
    "        self.m = input_shape[1]  # number of examples in training data\n",
    "        # `params` store weights and bias in a python dictionary\n",
    "        self.params = initialize_parameters(input_shape[0], n_out, ini_type)  # initialize weights and bias\n",
    "        self.Z = np.zeros((self.params['W'].shape[0], input_shape[1]))  # create space for resultant Z output\n",
    "\n",
    "    def forward(self, A_prev):\n",
    "\n",
    "        self.A_prev = A_prev  # store the Activations/Training Data coming in\n",
    "        self.Z = np.dot(self.params['W'], self.A_prev) + self.params['b']  # compute the linear function\n",
    "\n",
    "    def backward(self, upstream_grad):\n",
    "\n",
    "\n",
    "        # derivative of Cost w.r.t W\n",
    "        self.dW = np.dot(upstream_grad, self.A_prev.T)\n",
    "\n",
    "        # derivative of Cost w.r.t b, sum across rows\n",
    "        self.db = np.sum(upstream_grad, axis=1, keepdims=True)\n",
    "\n",
    "        # derivative of Cost w.r.t A_prev\n",
    "        self.dA_prev = np.dot(self.params['W'].T, upstream_grad)\n",
    "\n",
    "    def update_params(self, learning_rate=0.1):\n",
    "\n",
    "\n",
    "        self.params['W'] = self.params['W'] - learning_rate * self.dW  # update weights\n",
    "        self.params['b'] = self.params['b'] - learning_rate * self.db  # update bias(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SigmoidLayer:\n",
    "\n",
    "\n",
    "    def __init__(self, shape):\n",
    "\n",
    "        self.A = np.zeros(shape)  # create space for the resultant activations\n",
    "\n",
    "    def forward(self, Z):\n",
    "\n",
    "        self.A = 1 / (1 + np.exp(-Z))  # compute activations\n",
    "\n",
    "    def backward(self, upstream_grad):\n",
    "\n",
    "        # couple upstream gradient with local gradient, the result will be sent back to the Linear layer\n",
    "        self.dZ = upstream_grad * self.A*(1-self.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y, Y_hat):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (1 / (2 * m)) * np.sum(np.square(Y - Y_hat))\n",
    "    cost = np.squeeze(cost)  # remove extraneous dimensions to give just a scalar\n",
    "\n",
    "    dY_hat = -1 / m * (Y - Y_hat)  # derivative of the squared error cost function\n",
    "\n",
    "    return cost, dY_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training constants\n",
    "learning_rate = 1\n",
    "number_of_epochs = 5000\n",
    "\n",
    "np.random.seed(48) # set seed value so that the results are reproduceable\n",
    "                  # (weights will now be initailzaed to the same pseudo-random numbers, each time)\n",
    "\n",
    "\n",
    "# Our network architecture has the shape: \n",
    "#                   (input)--> [Linear->Sigmoid] -> [Linear->Sigmoid] -->(output)  \n",
    "\n",
    "#------ LAYER-1 ----- define hidden layer that takes in training data \n",
    "Z1 = LinearLayer(input_shape=X_train.shape, n_out=3, ini_type='xavier')\n",
    "A1 = SigmoidLayer(Z1.Z.shape)\n",
    "\n",
    "#------ LAYER-2 ----- define output layer that take is values from hidden layer\n",
    "Z2= LinearLayer(input_shape=A1.A.shape, n_out=1, ini_type='xavier')\n",
    "A2= SigmoidLayer(Z2.Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = [] # initially empty list, this will store all the costs after a certian number of epochs\n",
    "\n",
    "# Start training\n",
    "for epoch in range(number_of_epochs):\n",
    "    \n",
    "    # ------------------------- forward-prop -------------------------\n",
    "    Z1.forward(X_train)\n",
    "    A1.forward(Z1.Z)\n",
    "    \n",
    "    Z2.forward(A1.A)\n",
    "    A2.forward(Z2.Z)\n",
    "    \n",
    "    # ---------------------- Compute Cost ----------------------------\n",
    "    cost, dA2 = compute_cost(Y=Y_train, Y_hat=A2.A)\n",
    "    \n",
    "    # print and store Costs every 100 iterations.\n",
    "    if (epoch % 100) == 0:\n",
    "        #print(\"Cost at epoch#\" + str(epoch) + \": \" + str(cost))\n",
    "        print(\"Cost at epoch#{}: {}\".format(epoch, cost))\n",
    "        costs.append(cost)\n",
    "    \n",
    "    # ------------------------- back-prop ----------------------------\n",
    "    A2.backward(dA2)\n",
    "    Z2.backward(A2.dZ)\n",
    "    \n",
    "    A1.backward(Z2.dA_prev)\n",
    "    Z1.backward(A1.dZ)\n",
    "    \n",
    "    # ----------------------- Update weights and bias ----------------\n",
    "    Z2.update_params(learning_rate=learning_rate)\n",
    "    Z1.update_params(learning_rate=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
